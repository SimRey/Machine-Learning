{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Scaling</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some algorithms, like neural networks and SVMs, are very sensitive to the scaling of the data. Therefore, a common practice is to adjust the features so that the data representation is more suitable for these algorithms. Often, this is a simple per-feature rescaling and shift of the data.\n",
    "\n",
    "There are many scaling algorithms, the most used ones are:\n",
    "- Standarization\n",
    "- Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standarization\n",
    "\n",
    "Standardization or Z-Score Normalization is the transformation of features by subtracting from mean and dividing by standard deviation. This is often called as Z-score.\n",
    "\n",
    "$$x_{new} = \\frac{x-\\mu}{\\sigma}$$\n",
    "\n",
    "The StandardScaler in scikit-learn ensures that for each feature the mean is 0 and the variance is 1, bringing all features to the same magnitude. However, this scaling does not ensure any particular minimum and maximum values for the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.522e+01, 3.062e+01, 1.034e+02, 7.169e+02, 1.048e-01, 2.087e-01,\n",
       "       2.550e-01, 9.429e-02, 2.128e-01, 7.152e-02, 2.602e-01, 1.205e+00,\n",
       "       2.362e+00, 2.265e+01, 4.625e-03, 4.844e-02, 7.359e-02, 1.608e-02,\n",
       "       2.137e-02, 6.142e-03, 1.752e+01, 4.279e+01, 1.287e+02, 9.150e+02,\n",
       "       1.417e-01, 7.917e-01, 1.170e+00, 2.356e-01, 4.089e-01, 1.409e-01])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=1)\n",
    "\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the data has different orders of magnitud, so a scaler is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train) # The scaler has to be fitted only with the training data,\n",
    "                    # if fitted with the test data the model will not be fairly evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformed shape: (426, 30)\n",
      "per-feature mean before scaling:\n",
      " [1.41195047e+01 1.93320423e+01 9.19253991e+01 6.56126056e+02\n",
      " 9.64633333e-02 1.04575516e-01 8.85219054e-02 4.88310070e-02\n",
      " 1.80740845e-01 6.28127700e-02 4.11284742e-01 1.22736878e+00\n",
      " 2.89862465e+00 4.14559061e+01 7.02072066e-03 2.58229225e-02\n",
      " 3.18465390e-02 1.18348709e-02 2.04109296e-02 3.86226596e-03\n",
      " 1.63016291e+01 2.56733568e+01 1.07488521e+02 8.89164085e+02\n",
      " 1.32226502e-01 2.55249671e-01 2.70065559e-01 1.14623918e-01\n",
      " 2.87710798e-01 8.38468075e-02]\n",
      "per-feature std before scaling:\n",
      " [3.59928640e+00 4.34952000e+00 2.48120364e+01 3.61164531e+02\n",
      " 1.37973673e-02 5.09370908e-02 7.95203274e-02 3.90652971e-02\n",
      " 2.71231641e-02 6.77976825e-03 2.89636324e-01 5.83210517e-01\n",
      " 2.09856063e+00 4.92091215e+01 3.08773296e-03 1.81004824e-02\n",
      " 2.94992779e-02 6.34786633e-03 7.79220571e-03 2.83165677e-03\n",
      " 4.98490876e+00 6.26863717e+00 3.45686879e+01 5.92364546e+02\n",
      " 2.22082780e-02 1.54248636e-01 2.03768939e-01 6.66352413e-02\n",
      " 5.76639437e-02 1.68492387e-02]\n",
      "per-feature mean after scaling:\n",
      " [-2.19060555e-15  7.05161021e-16  2.87224600e-15 -1.03594754e-16\n",
      " -1.87552113e-15 -1.26815616e-15  3.35412449e-16  2.83549918e-16\n",
      "  1.87330589e-15 -1.89754316e-15  3.63819564e-16 -8.30321727e-16\n",
      "  9.75224075e-16 -4.77187408e-16  8.41006972e-16 -2.69997900e-16\n",
      "  1.98067957e-17 -1.86467300e-15 -1.82743752e-15  2.48106178e-16\n",
      " -6.92195388e-16  1.05249664e-15 -1.12221135e-15  1.50375278e-16\n",
      "  8.74470032e-15  4.14900248e-16 -3.09090260e-16  8.83487336e-17\n",
      " -1.44902348e-16  6.40072241e-16]\n",
      "per-feature std after scaling:\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# transform data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "\n",
    "# print dataset properties before and after scaling\n",
    "print(\"transformed shape: {}\".format(X_train_scaled.shape))\n",
    "print(\"per-feature mean before scaling:\\n {}\".format(X_train.mean(axis=0)))\n",
    "print(\"per-feature std before scaling:\\n {}\".format(X_train.std(axis=0)))\n",
    "\n",
    "print(\"per-feature mean after scaling:\\n {}\".format(X_train_scaled.mean(axis=0))) # should be zeros\n",
    "print(\"per-feature std after scaling:\\n {}\".format(X_train_scaled.std(axis=0))) # Should be ones\n",
    "\n",
    "# To avoid .transform() we can use fit_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0086086  -0.03878258  0.00699749 -0.01362775 -0.02971919 -0.01832081\n",
      "  0.013881    0.00897744  0.06176414 -0.00889741 -0.08397601 -0.0717422\n",
      " -0.06174637 -0.0904677   0.02610589 -0.07579365  0.00636354 -0.02427944\n",
      "  0.06708252 -0.09465667 -0.02589349  0.00245419 -0.02616429 -0.05763985\n",
      "  0.02545833 -0.0253996   0.04145461 -0.00105661  0.16317791  0.02338157]\n",
      "[0.91029049 0.95069544 0.91126505 0.88994655 1.07117801 1.13607535\n",
      " 1.00638752 0.96940983 1.03724313 1.15261976 0.81050016 0.7555601\n",
      " 0.83969644 0.63978344 0.88106901 0.9511862  1.08631385 0.8794542\n",
      " 1.21973437 0.69477324 0.86860917 0.91620326 0.87952945 0.82943534\n",
      " 1.1040482  1.07384599 1.08777938 0.94132584 1.25430308 1.25880753]\n"
     ]
    }
   ],
   "source": [
    "# Now the test data can be transformed\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(X_test_scaled.mean(axis=0)) \n",
    "print(X_test_scaled.std(axis=0)) \n",
    "\n",
    "# comes close to zero and one, it's clear it won't be zero ore one due to the fact that is transformed with train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note that the the y values are not necessary to be scaled!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Normalization is the transformation of features by subtracting min and max values from the data.\n",
    "\n",
    "$$x_{new} = \\frac{x-x_{min}}{x_{max} - x_{min}}$$\n",
    "\n",
    "This scales the range to [0, 1] or sometimes [-1, 1] (when MaxAbsScaler). Geometrically speaking, transformation squishes the n-dimensional data into an n-dimensional unit hypercube. Normalization is useful when there are no outliers as it cannot cope up with them. Usually, we would scale age and not incomes because only a few people have high incomes but the age is close to uniform.\n",
    "\n",
    "The MinMaxScaler, on the other hand, shifts the data such that all features are exactly between 0 and 1. For the two-dimensional dataset this means all of the data is contained within the rectangle created by the x-axis between 0 and 1 and the y-axis between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.522e+01, 3.062e+01, 1.034e+02, 7.169e+02, 1.048e-01, 2.087e-01,\n",
       "       2.550e-01, 9.429e-02, 2.128e-01, 7.152e-02, 2.602e-01, 1.205e+00,\n",
       "       2.362e+00, 2.265e+01, 4.625e-03, 4.844e-02, 7.359e-02, 1.608e-02,\n",
       "       2.137e-02, 6.142e-03, 1.752e+01, 4.279e+01, 1.287e+02, 9.150e+02,\n",
       "       1.417e-01, 7.917e-01, 1.170e+00, 2.356e-01, 4.089e-01, 1.409e-01])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=1)\n",
    "\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the data has different orders of magnitud, so a scaler is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(X_train) # The scaler has to be fitted only with the training data,\n",
    "                    # if fitted with the test data the model will not be fairly evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformed shape: (426, 30)\n",
      "per-feature min before scaling:\n",
      " [1.41195047e+01 1.93320423e+01 9.19253991e+01 6.56126056e+02\n",
      " 9.64633333e-02 1.04575516e-01 8.85219054e-02 4.88310070e-02\n",
      " 1.80740845e-01 6.28127700e-02 4.11284742e-01 1.22736878e+00\n",
      " 2.89862465e+00 4.14559061e+01 7.02072066e-03 2.58229225e-02\n",
      " 3.18465390e-02 1.18348709e-02 2.04109296e-02 3.86226596e-03\n",
      " 1.63016291e+01 2.56733568e+01 1.07488521e+02 8.89164085e+02\n",
      " 1.32226502e-01 2.55249671e-01 2.70065559e-01 1.14623918e-01\n",
      " 2.87710798e-01 8.38468075e-02]\n",
      "per-feature max before scaling:\n",
      " [3.59928640e+00 4.34952000e+00 2.48120364e+01 3.61164531e+02\n",
      " 1.37973673e-02 5.09370908e-02 7.95203274e-02 3.90652971e-02\n",
      " 2.71231641e-02 6.77976825e-03 2.89636324e-01 5.83210517e-01\n",
      " 2.09856063e+00 4.92091215e+01 3.08773296e-03 1.81004824e-02\n",
      " 2.94992779e-02 6.34786633e-03 7.79220571e-03 2.83165677e-03\n",
      " 4.98490876e+00 6.26863717e+00 3.45686879e+01 5.92364546e+02\n",
      " 2.22082780e-02 1.54248636e-01 2.03768939e-01 6.66352413e-02\n",
      " 5.76639437e-02 1.68492387e-02]\n",
      "per-feature min after scaling:\n",
      " [0.33785341 0.32539879 0.33263354 0.21744477 0.39571484 0.31870237\n",
      " 0.2074084  0.24269884 0.37747902 0.2762639  0.10733029 0.19164798\n",
      " 0.10091055 0.06472551 0.18043039 0.17702799 0.08042055 0.22418774\n",
      " 0.20939369 0.10252014 0.29781676 0.36389544 0.28426974 0.17301516\n",
      " 0.40319951 0.25033732 0.23082526 0.39389662 0.31157509 0.30663677]\n",
      "per-feature max after scaling:\n",
      " [0.17034817 0.14709232 0.17146041 0.1531981  0.1245587  0.19054725\n",
      " 0.18631754 0.19416152 0.13698568 0.14897315 0.10502822 0.128892\n",
      " 0.09888143 0.09191129 0.10496424 0.13594258 0.07449313 0.12024752\n",
      " 0.15007811 0.0978282  0.17733578 0.16707455 0.17216339 0.14558704\n",
      " 0.14665706 0.16939045 0.17416149 0.22898708 0.13703409 0.18041802]\n"
     ]
    }
   ],
   "source": [
    "# transform data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "\n",
    "# print dataset properties before and after scaling\n",
    "print(\"transformed shape: {}\".format(X_train_scaled.shape))\n",
    "print(\"per-feature min before scaling:\\n {}\".format(X_train.mean(axis=0)))\n",
    "print(\"per-feature max before scaling:\\n {}\".format(X_train.std(axis=0)))\n",
    "\n",
    "print(\"per-feature min after scaling:\\n {}\".format(X_train_scaled.mean(axis=0))) \n",
    "print(\"per-feature max after scaling:\\n {}\".format(X_train_scaled.std(axis=0))) \n",
    "\n",
    "# To avoid .transform() we can use fit_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33931987 0.31969417 0.33383333 0.21535703 0.39201306 0.31521139\n",
      " 0.20999467 0.24444191 0.38593982 0.27493842 0.09851044 0.18240098\n",
      " 0.09480498 0.0564105  0.18317057 0.1667244  0.08089459 0.2212682\n",
      " 0.2194613  0.09326005 0.29322492 0.36430547 0.27976521 0.16462354\n",
      " 0.40693316 0.24603488 0.23804506 0.39365468 0.33393603 0.31085522]\n",
      "[0.15506631 0.13984    0.15624588 0.13633812 0.13342454 0.21647603\n",
      " 0.18750765 0.18822208 0.14208745 0.1717094  0.08512539 0.09738565\n",
      " 0.08303039 0.05880332 0.09248074 0.1293067  0.08092291 0.10575218\n",
      " 0.18305543 0.06796841 0.15403548 0.15307425 0.15142278 0.12075504\n",
      " 0.16191646 0.18189925 0.18944927 0.21555146 0.17188228 0.22711156]\n"
     ]
    }
   ],
   "source": [
    "# Now the test data can be transformed\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(X_test_scaled.mean(axis=0)) \n",
    "print(X_test_scaled.std(axis=0)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we can see...\n",
    "\n",
    "Normalization and Standarization work very similar, the only difference is in how they scale data.\n",
    "\n",
    "#### Standardization vs. Normalization: When to Use Each\n",
    "\n",
    "Typically we normalize data when performing some type of analysis in which we have multiple variables that are measured on different scales and we want each of the variables to have the same range. This prevents one variable from being overly influential, especially if it’s measured in different units (i.e. if one variable is measured in inches and another is measured in yards).\n",
    "\n",
    "\n",
    "On the other hand, we typically standardize data when we’d like to know how many standard deviations each value in a dataset lies from the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust scaling\n",
    "\n",
    "$$x_{new} = \\frac{x-median}{IQR}$$\n",
    "\n",
    "The RobustScaler works similarly to the StandardScaler in that it ensures statistical properties for each feature that guarantee that they are on the same scale. However, the RobustScaler uses the median and quartiles,1 instead of mean and variance. This makes the RobustScaler ignore data points that are very different from the rest (like measurement errors). These odd data points are also called outliers, and can lead to trouble for other scaling techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[1, 2, 3],\n",
    "                    [4, 5, 6],\n",
    "                    [7, 999, 999]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the data has outliers, so a scaler is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobustScaler()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = RobustScaler()\n",
    "\n",
    "scaler.fit(X_train) # The scaler has to be fitted only with the training data,\n",
    "                    # if fitted with the test data the model will not be fairly evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.        , -0.00601805, -0.0060241 ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 1.        ,  1.99398195,  1.9939759 ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "X_train_scaled\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20a9e06a1eee47c4abbed4ec8225ad91d78d9800d202b71b6b0a6e47016c6abd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
